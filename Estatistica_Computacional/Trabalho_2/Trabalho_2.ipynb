{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a630d62",
   "metadata": {},
   "source": [
    "Aluno: Felipe Adrian Moreno Vera\n",
    "Materia: Estadistica Computacional\n",
    "Doutorado - EMAp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5e9490",
   "metadata": {},
   "source": [
    "### Setting global variables and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47eb3140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install matplotlib\n",
    "# !pip install numpy\n",
    "# !pip install scipy\n",
    "# !pip install gif\n",
    "# !pip install seaborn\n",
    "# !pip install pandas\n",
    "# !pip install sklearn\n",
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28131906",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import t, describe as stat_describe\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import invgamma, invwishart, invgauss, norminvgauss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb904b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 13 # number of samples\n",
    "n = 3 # dimensions of samples\n",
    "N = n*q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebeb38b",
   "metadata": {},
   "source": [
    "## Paper 3: Blocked Gibbs sampling\n",
    "\n",
    "The so-called Gibbs sampler is a work horse of Computational Statistics.  \n",
    "It depends on decomposing a target distribution into conditional densities from which new values of a given coordinate can be drawn.\n",
    "\n",
    "One of the difficulties one might encounter with the Gibbs sampler is that it might be slow to converge, specially in highly-correlated targets.  \n",
    "In Statistics, multilevel models (also called hierarchical or random effects) are extremely useful in modelling data coming from stratified structures (e.g. individuals within a city and cities within a state) and typically present highly correlated posterior distributions.\n",
    "\n",
    "One way to counteract the correlation between coordinates in the Gibbs sampler is to __block__ them together, and sample correlated coordinates jointly.\n",
    "\n",
    "For this assigment you are referred to the 2009 _Journal of Computational and Graphical Statistics_ paper by Tan and Hobert."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ec1264",
   "metadata": {},
   "source": [
    "* Precisely describe the so-called blocked Gibbs sampler;  \n",
    "  __Hint:__ you do not need to describe theoretical properties of the algorithm given in this paper; a general description of the algorithm should suffice. \n",
    "  \n",
    "__sol__:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df31380",
   "metadata": {},
   "source": [
    "In simple words, the main difference is that Blocked Gibbs Sampler groups two or more variables together (called __block__) and samples from their __joint distribution conditioned on all other variables__, rather than sampling from each one individually (Gibbs Sampler). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176a6311",
   "metadata": {},
   "source": [
    "\n",
    "* Explain the advantages -- both theoretical and practical -- of a clever blocking scheme;\n",
    "\n",
    "__sol__:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937efc99",
   "metadata": {},
   "source": [
    "The main advantage is that Block Gibbs Sampler ensure that the estimator is unbiased and the strong law of large numbers (SLLN) implies that it converges almost surely to $E_{\\pi}g$; that is, it is also strongly consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1892ee1a",
   "metadata": {},
   "source": [
    "* Would it be possible to apply the \"simple\" Gibbs sampler in this example? Why?\n",
    "\n",
    "__sol__:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5870f4f6",
   "metadata": {},
   "source": [
    "#### Implementation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04270b31",
   "metadata": {},
   "source": [
    "The model described is $Y_{ij} = \\theta_i + \\epsilon_{ij}$, $i=1,\\dots,q$, $j=1,\\dots,n$. Where $\\theta_1,\\dots,\\theta_q \\sim \\mathcal{N}(\\mu, \\sigma^2_{\\theta})$, $\\epsilon_{ij} \\sim \\mathcal{N}(0, \\sigma^2_{\\epsilon})$, and $(\\mu, \\sigma^2_{\\theta}, \\sigma^2_{\\epsilon})$ are unknown parameter. So, we have the following equation:\n",
    "\n",
    "\\begin{equation}\n",
    "\\pi_{a,b}(\\mu, \\sigma^2_{\\theta}, \\sigma^2_{\\epsilon}) = (\\sigma^2_{\\theta})^{-(a+1)} (\\sigma^2_{\\epsilon})^{-(b+1)}\n",
    "\\end{equation}\n",
    "\n",
    "Where $a$ and $b$ are known hyper-parameters. We define as posteriori:\n",
    "\n",
    "\\begin{equation}\n",
    "\\pi(\\theta, \\mu, \\sigma^2_{\\theta}, \\sigma^2_{\\epsilon})  \\propto f(y|\\theta, \\mu, \\sigma^2_{\\theta}, \\sigma^2_{\\epsilon})\\, f(\\theta|\\mu, \\sigma^2_{\\theta}, \\sigma^2_{\\epsilon}) \\, \\pi_{a,b}(\\mu, \\sigma^2_{\\theta}, \\sigma^2_{\\epsilon})\n",
    "\\end{equation}\n",
    "\n",
    "Where:\n",
    "\n",
    "\\begin{equation}\n",
    "f(y|\\theta, \\mu, \\sigma^2_{\\theta}, \\sigma^2_{\\epsilon}) = \\prod^q_{i=1} \\prod^{n_i}_{j=1} \\frac{1}{\\sqrt{2\\pi \\sigma^2_{\\epsilon}}} \\exp \\{ -\\frac{1}{2 \\sigma^2_{\\epsilon} } (y_{ij} - \\mu )^2 \\}\n",
    "\\end{equation}\n",
    "\n",
    "And:\n",
    "\n",
    "\\begin{equation}\n",
    "f(\\theta|\\mu, \\sigma^2_{\\theta}, \\sigma^2_{\\epsilon}) = \\prod^q_{i=1} \\frac{1}{\\sqrt{2\\pi \\sigma^2_{\\theta}}} \\exp \\{ -\\frac{1}{2 \\sigma^2_{\\theta} } (\\theta_i - \\mu )^2 \\}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Folowwing the ideas, that function was replaced by:\n",
    "\n",
    "\\begin{equation}\n",
    "f_y(\\theta, \\epsilon) = g(\\theta, \\mu, \\sigma^2_{\\theta}, \\sigma^2_{\\epsilon}) \\, \\pi(\\theta, \\mu, \\sigma^2_{\\theta}, \\sigma^2_{\\epsilon})\n",
    "\\end{equation}\n",
    "\n",
    "Where $g()$ is our new function to estimate and $\\pi()$ is our density function respectively, as was defined in paper. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5ed321",
   "metadata": {},
   "source": [
    "Results in Hobert and Casella (1996) show that the posterior is proper if and only if:\n",
    "\n",
    "\\begin{equation}\n",
    "a<0\\,\\,,a+\\frac{q}{2}>\\frac{1}{2}\\, \\text{,  }\\, a+b> \\frac{1-N}{2}\n",
    "\\end{equation}\n",
    "\n",
    "from this, we will take $a=-\\frac{1}{2}$ and $b=0$ (also, they recommend this values in appendix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a389ca6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = -1/2\n",
    "b = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd9bb423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pi_func(sigma_theta, sigma_epsilon, a=-1/2, b=-1/2):\n",
    "    return (sigma_theta)**(-1*(a+1)) * (sigma_epsilon)**(-1*(b+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cfa52b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_theta = 4.8\n",
    "sigma_theta = 1/2\n",
    "sigma_epsilon = 1/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4258f461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_Y(Y):\n",
    "    #n_i = Y.shape[1]\n",
    "    #for i in range(n_i):\n",
    "    #    Y[:,i] = Y[:,i]/(i+1)\n",
    "    return np.sum(Y, axis=1)/Y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c0af01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(q, n, mu_theta, sigma_theta, sigma_epsilon):\n",
    "    epsilon = np.random.normal(0.0, sigma_epsilon, size=(q,n))\n",
    "    thetas = np.random.normal(mu_theta, sigma_theta, size=(q,1))\n",
    "    return thetas, epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0846ec34",
   "metadata": {},
   "source": [
    "We got a similar mean of original data in section 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f24a0f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.74055008, 4.49271485, 4.10473472],\n",
       "       [5.20087365, 5.67687901, 6.36366368],\n",
       "       [4.94134587, 5.46195534, 5.47956828],\n",
       "       [3.36147526, 4.53845234, 3.80662217],\n",
       "       [4.6153383 , 5.49925796, 6.33969921],\n",
       "       [6.32656061, 5.54915835, 4.81413023],\n",
       "       [4.30317545, 3.66239483, 3.94178328],\n",
       "       [5.79283008, 4.76295255, 5.69454489],\n",
       "       [4.96550562, 4.73241027, 5.16218243],\n",
       "       [4.50715686, 3.96954572, 4.03249638],\n",
       "       [5.34975109, 5.47738059, 5.38247253],\n",
       "       [4.84681837, 5.5811558 , 3.65644711],\n",
       "       [5.66371926, 5.98857031, 5.61674286]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thetas, epsilon = generate_data(q, n, mu_theta, sigma_theta, sigma_epsilon)\n",
    "Y = thetas + epsilon\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7edf723c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.44599988, 5.74713878, 5.29428983, 3.90218326, 5.48476515,\n",
       "       5.56328306, 3.96911785, 5.41677584, 4.95336611, 4.16973299,\n",
       "       5.4032014 , 4.69480709, 5.75634414])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_i = get_mean_Y(Y)\n",
    "Y_i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e5655c",
   "metadata": {},
   "source": [
    "We calculate the global mean:\n",
    "\n",
    "\\begin{equation}\n",
    "\\bar{y} = \\frac{1}{N} \\sum^q_{i=1} \\sum^{n_i}_{j=1} y_{i,j}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a38c1dcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.98469272147464, 4.8531295229404465)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(Y), np.mean(thetas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7587b32",
   "metadata": {},
   "source": [
    "Besides, the $SST$:\n",
    "\n",
    "\\begin{equation}\n",
    "SST = n_i \\sum^q_{i=1} (y_{i} - \\bar{y} )^2\n",
    "\\end{equation}\n",
    "\n",
    "Where:\n",
    "\n",
    "\\begin{equation}\n",
    "y_{i} = \\frac{1}{n_i} \\sum_{j} y_{i,j}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2bcf49d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.44599988, 5.74713878, 5.29428983, 3.90218326, 5.48476515,\n",
       "       5.56328306, 3.96911785, 5.41677584, 4.95336611, 4.16973299,\n",
       "       5.4032014 , 4.69480709, 5.75634414])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_i = get_mean_Y(Y)\n",
    "Y_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2e494704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.385664797927483"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n*np.sum(np.square(Y_i - np.mean(Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6a8a39",
   "metadata": {},
   "source": [
    "Besides, the $SSE$:\n",
    "\n",
    "\\begin{equation}\n",
    "SSE = \\sum^q_{i=1} \\sum^{n_i}_{j=1}(y_{i,j} - \\bar{y}_{i} )^2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aae515ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.526264596775054"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum( [ n*np.sum( np.square(Y[i,:] - Y_i[i]) ) for i in range(Y.shape[0])] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a743f90",
   "metadata": {},
   "source": [
    "* Implement the blocked Gibbs sampler discussed in the paper in order to fit the model of Section 1 to the data described in Section 5 therein.\n",
    "    \n",
    " __sol__:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cd60ba",
   "metadata": {},
   "source": [
    "We use the gibbs sampling idea from previous work but changing some conditions: this block gibbs sampler is a two-variable Gibbs sampler that updates $\\sigma^2 = (\\sigma^2_{\\theta}, \\sigma^2_{\\epsilon})$ and $\\xi = (\\mu, \\theta)$, so in each step we have:\n",
    "\n",
    "* Let's supose that we have the iteration $k$ as $(\\sigma^2_k, \\xi_k)$.\n",
    "* One iteration updates $\\sigma^2_{k+1}$ conditional on $\\xi_k$.\n",
    "* Then, updates $\\xi_{k+1}$ conditional on $\\sigma^2_{k+1}$.\n",
    "* And so on.\n",
    "\n",
    "So, if we sample $(\\sigma^{2}_0, \\xi_0), (\\sigma^{2}_1, \\xi_1), \\dots\\,$ we would estimate $E_{\\pi}g$ using MonteCarlo:\n",
    "\n",
    "\\begin{equation}\n",
    "\\bar{g}_N = \\frac{1}{N} \\sum _{n=0} ^{N-1} g(\\sigma^{2}_n, \\xi_n)\n",
    "\\end{equation}\n",
    "\n",
    "Also, we know that:\n",
    "\n",
    "\\begin{equation}\n",
    "\\pi(\\theta, \\mu, \\sigma^2_{\\theta}, \\sigma^2_{\\epsilon}) = \\pi(\\sigma^2_{\\epsilon} | \\theta) \\, \\pi(\\mu | \\theta) \\, \\, \\pi(\\sigma^2_{\\theta}|\\theta)\n",
    "\\end{equation}\n",
    "\n",
    "Where:\n",
    "* $\\pi(\\sigma^2_{\\epsilon} | \\theta)$ is an inverse gamma density.\n",
    "* $\\pi(\\sigma^2_{\\theta}|\\theta)$ is an inverse gamma density.\n",
    "* $\\pi(\\mu | \\theta)$ is an inverse normal density."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcb3afd",
   "metadata": {},
   "source": [
    "Since we know that $\\sigma^2_{\\theta}$, $\\sigma^2_{\\epsilon}$, and $\\xi$ are independent, we have:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma^2_{\\theta}|\\xi \\sim IG(\\frac{q}{2} + a, \\frac{1}{2} \\sum_i (\\theta_i - \\mu))\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma^2_{\\epsilon}|\\xi \\sim IG(\\frac{M}{2} + b, \\frac{1}{2} \\sum_{i,j} (y_{i,j} - \\theta_i))\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d98f722",
   "metadata": {},
   "source": [
    "We know:\n",
    "\n",
    "\\begin{equation}\n",
    "E(\\mu | \\sigma^2) = E(\\mu | \\sigma^2_{\\theta}, \\sigma^2_{\\epsilon}) = \\frac{1}{t} \\sum^q_{i=1} \\frac{n_i \\bar{y}_i}{\\sigma^2_{\\epsilon} + n_i \\sigma^2_{\\theta}}\n",
    "\\end{equation}\n",
    "\n",
    "Where:\n",
    "\n",
    "\\begin{equation}\n",
    "t = \\sum^q_{i=1} \\frac{n_i}{\\sigma^2_{\\epsilon} + n_i \\sigma^2_{\\theta}}\n",
    "\\end{equation}\n",
    "\n",
    "So, we can estimate $\\mu$ using that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448d4974",
   "metadata": {},
   "source": [
    "For our implementation, we will use _thin_ which means __Thinning__. This consists in picking separated points from the sample, at each k-th step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecae50f",
   "metadata": {},
   "source": [
    "We will estimate $\\mu$, $\\sigma^2_{\\theta}$, and $\\sigma^2_{\\epsilon}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42d4afd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_t(q, n, sigma_epsilon, sigma_theta):\n",
    "    t = 0\n",
    "    for i in range(q):\n",
    "        t = t + n/(sigma_epsilon**2 + n*sigma_theta**2)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e291d413",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gibbs_sampler(Y, thetas, mu_theta, a=-1/2, b=0, iterations=1000,thin=500):\n",
    "    mat = np.zeros((iterations,3))\n",
    "    n = Y.shape[1]\n",
    "    q = Y.shape[0]\n",
    "    M = q*n\n",
    "    for i in range(iterations):\n",
    "        # updates sigmas\n",
    "        s_theta = np.sqrt(abs(invgauss.rvs( q/2 + a, 1/2*np.sum(thetas - mu_theta) ) ))\n",
    "        s_epsilon = np.sqrt(abs(invgauss.rvs(M/2 + b, 1/2*np.sum(Y - thetas) ) ))\n",
    "        # updates mu\n",
    "        t = get_t(q, M, s_epsilon, s_theta)\n",
    "        Y_i = get_mean_Y(Y)\n",
    "        mu_theta = 1/t*np.sum( M*Y_i/(s_epsilon**2 + M*s_theta**2) )\n",
    "        #mu_theta = np.mean(thetas)\n",
    "        #thetas, eps = generate_data(q, n, mu_theta, s_theta, s_epsilon)\n",
    "        #Y = thetas + eps\n",
    "        mat[i] = [mu_theta, s_theta, s_epsilon]\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "462d72d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def block_gibbs_sampler(q=13, n=3, mu_theta=4.8, sigma_theta=1/2, sigma_epsilon=1/2, a=-1/2, b=0, iterations = 1000):\n",
    "    \n",
    "    thetas, epsilon = generate_data(q, n, mu_theta, sigma_theta, sigma_epsilon)\n",
    "    Y_original = thetas + epsilon\n",
    "    # gibbs sampler\n",
    "    estimators = gibbs_sampler(Y_original, thetas, mu_theta, a=-1/2, b=0, iterations=iterations)\n",
    "    best = np.mean(np.square(estimators - np.array([mu_theta, sigma_theta, sigma_epsilon])), axis=1)\n",
    "    return estimators[np.where(best == best.min())][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516b8ec2",
   "metadata": {},
   "source": [
    "For data configuration, we define the following varaibles:  \n",
    "__note__ that with that configurations, the Block Gibbs Sampler should be __geometrically ergodic__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5074aa",
   "metadata": {},
   "source": [
    "* Assess convergence (or lack thereof) and mixing of the resulting chain.\n",
    "\n",
    "__sol__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "912214d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_theta_new, sigma_theta_new, sigma_epsilon_new = block_gibbs_sampler()\n",
    "thetas_generated, epsilon_generated = generate_data(q, n, mu_theta_new, sigma_theta_new, sigma_epsilon_new)\n",
    "Y_generated = thetas_generated + epsilon_generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f94e441b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.80971596074632, 4.8487635147474615)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(Y_generated), np.mean(thetas_generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64498aee",
   "metadata": {},
   "source": [
    "$SST$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0026c537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.4485404 , 4.58376308, 5.03630742, 4.31126815, 5.2142136 ,\n",
       "       4.88466518, 5.21461138, 4.09433676, 5.22855442, 4.99684387,\n",
       "       4.99362645, 4.90693769, 4.6126391 ])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_i_generated = get_mean_Y(Y_generated)\n",
    "Y_i_generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9b14b234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.856390289944261"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n*np.sum(np.square(Y_i_generated - np.mean(Y_generated)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f83cfa1",
   "metadata": {},
   "source": [
    "$SSE$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "97a3bcc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59.58876429295549"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum( [ n*np.sum( np.square(Y_generated[i,:] - Y_i_generated[i]) ) for i in range(Y_generated.shape[0])] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c95adee",
   "metadata": {},
   "source": [
    "* Confirm your results agree with those given by the original authors up to Monte Carlo error.\n",
    "\n",
    "__sol__:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e36d75",
   "metadata": {},
   "source": [
    "Our new estimator $\\tilde{g}_{R}$ derivated from $\\bar{g}_N$ is defiend as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\bar{g}_{N} = \\frac{1}{N} \\sum^{N-1}_{i=0} g(\\sigma^{2}_n, \\xi_n) = \\frac{1}{N} \\sum^{N-1}_{i=0} g(X_n)\n",
    "\\end{equation}\n",
    "\n",
    "Becomes:\n",
    "\n",
    "\\begin{equation}\n",
    "\\tilde{g}_{R} = \\frac{1}{\\tau_R} \\sum^{\\tau_R-1}_{i=0} g(X_n) = \\frac{\\sum^R_{t=1} S_t}{\\sum^R_{t=1} N_t}\n",
    "\\end{equation}\n",
    "\n",
    "Where:\n",
    "\n",
    "\\begin{equation}\n",
    "S_t = \\sum^{\\tau_t-1}_{n=\\tau_t-1} g(X_n)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "N_t = \\tau_t - \\tau_{t-1}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2ad1bb",
   "metadata": {},
   "source": [
    "Doing Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6aab2e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def St(t):\n",
    "    mu_theta_new, sigma_theta_new, sigma_epsilon_new = block_gibbs_sampler()\n",
    "    return np.array([mu_theta_new, sigma_theta_new, sigma_epsilon_new])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bc4c7fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Nt(t):\n",
    "    return t-(t-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "378c8bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "R = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ece021d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "S_total = []\n",
    "N_total = []\n",
    "\n",
    "for t in range(1, R+1, 1):\n",
    "    S_total.append( St(t) )\n",
    "    N_total.append( Nt(t) )\n",
    "\n",
    "S_total=np.array(S_total)\n",
    "N_total=np.array(N_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6b4b62b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.8, 0.5, 0.5)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu_theta, sigma_theta, sigma_epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bd471c98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.79954027, 0.61426285, 0.81381876])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_R = np.sum(S_total, axis=0)/np.sum(N_total)\n",
    "g_R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad950c33",
   "metadata": {},
   "source": [
    "Besides, we have the estimator $\\gamma^2$ defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\gamma^2 = \\frac{R}{\\tau_R ^2} \\sum^R_{t=1} (S_t - \\tilde{g}_{R}\\, N_t)^2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f288e671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22958115317057318"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma_square = 1/R* np.sum ( [ np.square(S_total[i,:] - g_R*N_total[i]) for i in range(S_total.shape[0])] )\n",
    "gamma_square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "aaaa6e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "R = 40000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cd006c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "S_total = []\n",
    "N_total = []\n",
    "\n",
    "for t in range(1, R+1, 1):\n",
    "    S_total.append( St(t) )\n",
    "    N_total.append( Nt(t) )\n",
    "\n",
    "S_total=np.array(S_total)\n",
    "N_total=np.array(N_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b5090c36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.8, 0.5, 0.5)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu_theta, sigma_theta, sigma_epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "61d90c82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.80015   , 0.61212311, 0.80865525])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_R = np.sum(S_total, axis=0)/np.sum(N_total)\n",
    "g_R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4feeae97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2269918042567983"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma_square = 1/R* np.sum ( [ np.square(S_total[i,:] - g_R*N_total[i]) for i in range(S_total.shape[0])] )\n",
    "gamma_square"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96751b6c",
   "metadata": {},
   "source": [
    "* Comment on the significance of geometric ergodicity for the blocked Gibbs sampler proposed by Tan-2009.\n",
    "\n",
    "__sol__:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cc90bd",
   "metadata": {},
   "source": [
    "If an estimator is __geometrically ergodicity__ and there is $\\alpha>0$ such that $E_{\\pi}|g|^{2+\\alpha}<\\infty$. Where $E_{\\pi}g$ is estimated by using the classical Monte Carlo.  \n",
    "So, in Block Gibbs sampling for a high value N of samples and dimension $q\\geq2$ we can calculate an asymptotic standard error for $\\bar{g}_N$. This means knowing some initial conditions, such as minimal number of samples to converge and minimal dimension data. Besides, the article proof that for that conditions (such as $q\\geq4$ and $M\\geq q+3$) the block gibbs is a strongly estimator."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
