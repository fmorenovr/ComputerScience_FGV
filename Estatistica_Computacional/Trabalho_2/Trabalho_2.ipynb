{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a77dc7b",
   "metadata": {},
   "source": [
    "Aluno: Felipe Adrian Moreno Vera\n",
    "Materia: Estadistica Computacional\n",
    "Doutorado - EMAp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892602dd",
   "metadata": {},
   "source": [
    "### Setting global variables and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "354c5711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install matplotlib\n",
    "# !pip install numpy\n",
    "# !pip install scipy\n",
    "# !pip install gif\n",
    "# !pip install seaborn\n",
    "# !pip install pandas\n",
    "# !pip install sklearn\n",
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbca8ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import t, describe as stat_describe\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import invgamma, invwishart, invgauss, norminvgauss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "974f2397",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 13 # number of samples\n",
    "n = 3 # dimensions of samples\n",
    "N = n*q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0eb635",
   "metadata": {},
   "source": [
    "## Paper 3: Blocked Gibbs sampling\n",
    "\n",
    "The so-called Gibbs sampler is a work horse of Computational Statistics.  \n",
    "It depends on decomposing a target distribution into conditional densities from which new values of a given coordinate can be drawn.\n",
    "\n",
    "One of the difficulties one might encounter with the Gibbs sampler is that it might be slow to converge, specially in highly-correlated targets.  \n",
    "In Statistics, multilevel models (also called hierarchical or random effects) are extremely useful in modelling data coming from stratified structures (e.g. individuals within a city and cities within a state) and typically present highly correlated posterior distributions.\n",
    "\n",
    "One way to counteract the correlation between coordinates in the Gibbs sampler is to __block__ them together, and sample correlated coordinates jointly.\n",
    "\n",
    "For this assigment you are referred to the 2009 _Journal of Computational and Graphical Statistics_ paper by Tan and Hobert."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32fa235",
   "metadata": {},
   "source": [
    "* Precisely describe the so-called blocked Gibbs sampler;  \n",
    "  __Hint:__ you do not need to describe theoretical properties of the algorithm given in this paper; a general description of the algorithm should suffice. \n",
    "  \n",
    "__sol__:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ea4f03",
   "metadata": {},
   "source": [
    "In simple words, the main difference is that Blocked Gibbs Sampler groups two or more variables together (called __block__) and samples from their __joint distribution conditioned on all other variables__, rather than sampling from each one individually (Gibbs Sampler). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddd0254",
   "metadata": {},
   "source": [
    "\n",
    "* Explain the advantages -- both theoretical and practical -- of a clever blocking scheme;\n",
    "\n",
    "__sol__:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c3c994",
   "metadata": {},
   "source": [
    "The main advantage is that Block Gibbs Sampler ensure that the estimator is unbiased and the strong law of large numbers (SLLN) implies that it converges almost surely to $E_{\\pi}g$; that is, it is also strongly consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1d0439",
   "metadata": {},
   "source": [
    "* Would it be possible to apply the \"simple\" Gibbs sampler in this example? Why?\n",
    "\n",
    "__sol__:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2616034",
   "metadata": {},
   "source": [
    "#### Implementation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a962259c",
   "metadata": {},
   "source": [
    "The model described is $Y_{ij} = \\theta_i + \\epsilon_{ij}$, $i=1,\\dots,q$, $j=1,\\dots,n$. Where $\\theta_1,\\dots,\\theta_q \\sim \\mathcal{N}(\\mu, \\sigma^2_{\\theta})$, $\\epsilon_{ij} \\sim \\mathcal{N}(0, \\sigma^2_{\\epsilon})$, and $(\\mu, \\sigma^2_{\\theta}, \\sigma^2_{\\epsilon})$ are unknown parameter. So, we have the following equation:\n",
    "\n",
    "\\begin{equation}\n",
    "\\pi_{a,b}(\\mu, \\sigma^2_{\\theta}, \\sigma^2_{\\epsilon}) = (\\sigma^2_{\\theta})^{-(a+1)} (\\sigma^2_{\\epsilon})^{-(b+1)}\n",
    "\\end{equation}\n",
    "\n",
    "Where $a$ and $b$ are known hyper-parameters. We define as posteriori:\n",
    "\n",
    "\\begin{equation}\n",
    "\\pi(\\theta, \\mu, \\sigma^2_{\\theta}, \\sigma^2_{\\epsilon})  \\propto f(y|\\theta, \\mu, \\sigma^2_{\\theta}, \\sigma^2_{\\epsilon})\\, f(\\theta|\\mu, \\sigma^2_{\\theta}, \\sigma^2_{\\epsilon}) \\, \\pi_{a,b}(\\mu, \\sigma^2_{\\theta}, \\sigma^2_{\\epsilon})\n",
    "\\end{equation}\n",
    "\n",
    "Where:\n",
    "\n",
    "\\begin{equation}\n",
    "f(y|\\theta, \\mu, \\sigma^2_{\\theta}, \\sigma^2_{\\epsilon}) = \\prod^q_{i=1} \\prod^{n_i}_{j=1} \\frac{1}{\\sqrt{2\\pi \\sigma^2_{\\epsilon}}} \\exp \\{ -\\frac{1}{2 \\sigma^2_{\\epsilon} } (y_{ij} - \\mu )^2 \\}\n",
    "\\end{equation}\n",
    "\n",
    "And:\n",
    "\n",
    "\\begin{equation}\n",
    "f(\\theta|\\mu, \\sigma^2_{\\theta}, \\sigma^2_{\\epsilon}) = \\prod^q_{i=1} \\frac{1}{\\sqrt{2\\pi \\sigma^2_{\\theta}}} \\exp \\{ -\\frac{1}{2 \\sigma^2_{\\theta} } (\\theta_i - \\mu )^2 \\}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Folowwing the ideas, that function was replaced by:\n",
    "\n",
    "\\begin{equation}\n",
    "f_y(\\theta, \\epsilon) = g(\\theta, \\mu, \\sigma^2_{\\theta}, \\sigma^2_{\\epsilon}) \\, \\pi(\\theta, \\mu, \\sigma^2_{\\theta}, \\sigma^2_{\\epsilon})\n",
    "\\end{equation}\n",
    "\n",
    "Where $g()$ is our new function to estimate and $\\pi()$ is our density function respectively, as was defined in paper. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79b2ea2",
   "metadata": {},
   "source": [
    "Results in Hobert and Casella (1996) show that the posterior is proper if and only if:\n",
    "\n",
    "\\begin{equation}\n",
    "a<0\\,\\,,a+\\frac{q}{2}>\\frac{1}{2}\\, \\text{,  }\\, a+b> \\frac{1-N}{2}\n",
    "\\end{equation}\n",
    "\n",
    "from this, we will take $a=-\\frac{1}{2}$ and $b=0$ (also, they recommend this values in appendix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11763997",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = -1/2\n",
    "b = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2350b6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pi_func(sigma_theta, sigma_epsilon, a=-1/2, b=-1/2):\n",
    "    return (sigma_theta)**(-1*(a+1)) * (sigma_epsilon)**(-1*(b+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b0013d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_theta = 4.8\n",
    "sigma_theta = 1/2\n",
    "sigma_epsilon = 1/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2353650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_Y(Y):\n",
    "    #n_i = Y.shape[1]\n",
    "    #for i in range(n_i):\n",
    "    #    Y[:,i] = Y[:,i]/(i+1)\n",
    "    return np.sum(Y, axis=1)/Y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "faa89559",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(q, n, mu_theta, sigma_theta, sigma_epsilon):\n",
    "    epsilon = np.random.normal(0.0, sigma_epsilon, size=(q,n))\n",
    "    thetas = np.random.normal(mu_theta, sigma_theta, size=(q,1))\n",
    "    return thetas, epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a1947f",
   "metadata": {},
   "source": [
    "We got a similar mean of original data in section 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "352c8f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.25433197, 5.15599977, 5.23662981],\n",
       "       [4.76206512, 5.15618728, 5.21902361],\n",
       "       [4.20455041, 4.25723101, 4.06970786],\n",
       "       [3.90264106, 3.70531302, 4.14149953],\n",
       "       [3.89815449, 4.26711131, 3.96631   ],\n",
       "       [5.33475761, 3.96883007, 4.48563916],\n",
       "       [5.71629972, 4.59558319, 5.98629365],\n",
       "       [5.08455474, 5.15645604, 5.38060025],\n",
       "       [4.46603545, 3.76986216, 4.23073778],\n",
       "       [4.61242925, 5.12904952, 3.78958416],\n",
       "       [3.93388431, 4.33176795, 4.18369523],\n",
       "       [4.56167101, 4.17484775, 4.61699277],\n",
       "       [4.25733711, 4.07386335, 3.76803068]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thetas, epsilon = generate_data(q, n, mu_theta, sigma_theta, sigma_epsilon)\n",
    "Y = thetas + epsilon\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f7abc5f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.21565385, 5.04575867, 4.17716309, 3.91648454, 4.0438586 ,\n",
       "       4.59640895, 5.43272552, 5.20720368, 4.15554513, 4.51035431,\n",
       "       4.1497825 , 4.45117051, 4.03307704])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_i = get_mean_Y(Y)\n",
    "Y_i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccccff6b",
   "metadata": {},
   "source": [
    "We calculate the global mean:\n",
    "\n",
    "\\begin{equation}\n",
    "\\bar{y} = \\frac{1}{N} \\sum^q_{i=1} \\sum^{n_i}_{j=1} y_{i,j}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1fb1d09e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.533475875593315, 4.590711037067509)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(Y), np.mean(thetas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1e23b3",
   "metadata": {},
   "source": [
    "Besides, the $SST$:\n",
    "\n",
    "\\begin{equation}\n",
    "SST = n_i \\sum^q_{i=1} (y_{i} - \\bar{y} )^2\n",
    "\\end{equation}\n",
    "\n",
    "Where:\n",
    "\n",
    "\\begin{equation}\n",
    "y_{i} = \\frac{1}{n_i} \\sum_{j} y_{i,j}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "19d170ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.868326920244009"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n*np.sum(np.square(get_mean_Y(Y) - np.mean(Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dd47db",
   "metadata": {},
   "source": [
    "Besides, the $SSE$:\n",
    "\n",
    "\\begin{equation}\n",
    "SSE = \\sum^q_{i=1} \\sum^{n_i}_{j=1}(y_{i,j} - \\bar{y}_{i} )^2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f94f98f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.666098300540602"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_ = []\n",
    "for i in range(Y.shape[0]):\n",
    "    sum_.append(n*np.sum( np.square(Y[i,:] - Y_i[i]) ) )\n",
    "np.sum(sum_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee42f179",
   "metadata": {},
   "source": [
    "* Implement the blocked Gibbs sampler discussed in the paper in order to fit the model of Section 1 to the data described in Section 5 therein.\n",
    "    \n",
    " __sol__:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ba214f",
   "metadata": {},
   "source": [
    "We use the gibbs sampling idea from previous work but changing some conditions: this block gibbs sampler is a two-variable Gibbs sampler that updates $\\sigma^2 = (\\sigma^2_{\\theta}, \\sigma^2_{\\epsilon})$ and $\\xi = (\\mu, \\theta)$, so in each step we have:\n",
    "\n",
    "* Let's supose that we have the iteration $k$ as $(\\sigma^2_k, \\xi_k)$.\n",
    "* One iteration updates $\\sigma^2_{k+1}$ conditional on $\\xi_k$.\n",
    "* Then, updates $\\xi_{k+1}$ conditional on $\\sigma^2_{k+1}$.\n",
    "* And so on.\n",
    "\n",
    "So, if we sample $(\\sigma^{2}_0, \\xi_0), (\\sigma^{2}_1, \\xi_1), \\dots\\,$ we would estimate $E_{\\pi}g$ using MonteCarlo:\n",
    "\n",
    "\\begin{equation}\n",
    "\\bar{g}_N = \\frac{1}{N} \\sum _{n=0} ^{N-1} g(\\sigma^{2}_n, \\xi_n)\n",
    "\\end{equation}\n",
    "\n",
    "Also, we know that:\n",
    "\n",
    "\\begin{equation}\n",
    "\\pi(\\theta, \\mu, \\sigma^2_{\\theta}, \\sigma^2_{\\epsilon}) = \\pi(\\sigma^2_{\\epsilon} | \\theta) \\, \\pi(\\mu | \\theta) \\, \\, \\pi(\\sigma^2_{\\theta}|\\theta)\n",
    "\\end{equation}\n",
    "\n",
    "Where:\n",
    "* $\\pi(\\sigma^2_{\\epsilon} | \\theta)$ is an inverse gamma density.\n",
    "* $\\pi(\\sigma^2_{\\theta}|\\theta)$ is an inverse gamma density.\n",
    "* $\\pi(\\mu | \\theta)$ is an inverse normal density."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe49e7c",
   "metadata": {},
   "source": [
    "Since we know that $\\sigma^2_{\\theta}$, $\\sigma^2_{\\epsilon}$, and $\\xi$ are independent, we have:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma^2_{\\theta}|\\xi \\sim IG(\\frac{q}{2} + a, \\frac{1}{2} \\sum_i (\\theta_i - \\mu))\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma^2_{\\epsilon}|\\xi \\sim IG(\\frac{M}{2} + b, \\frac{1}{2} \\sum_{i,j} (y_{i,j} - \\theta_i))\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a34e41d",
   "metadata": {},
   "source": [
    "We know:\n",
    "\n",
    "\\begin{equation}\n",
    "E(\\mu | \\sigma^2) = E(\\mu | \\sigma^2_{\\theta}, \\sigma^2_{\\epsilon}) = \\frac{1}{t} \\sum^q_{i=1} \\frac{n_i \\bar{y}_i}{\\sigma^2_{\\epsilon} + n_i \\sigma^2_{\\theta}}\n",
    "\\end{equation}\n",
    "\n",
    "Where:\n",
    "\n",
    "\\begin{equation}\n",
    "t = \\sum^q_{i=1} \\frac{n_i}{\\sigma^2_{\\epsilon} + n_i \\sigma^2_{\\theta}}\n",
    "\\end{equation}\n",
    "\n",
    "So, we can estimate $\\mu$ using that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac34a4e8",
   "metadata": {},
   "source": [
    "For our implementation, we will use _thin_ which means __Thinning__. This consists in picking separated points from the sample, at each k-th step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8069a45d",
   "metadata": {},
   "source": [
    "We will estimate $\\mu$, $\\sigma^2_{\\theta}$, and $\\sigma^2_{\\epsilon}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad6b3c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_t(q, n, sigma_epsilon, sigma_theta):\n",
    "    t = 0\n",
    "    for i in range(q):\n",
    "        t = t + n/(sigma_epsilon**2 + n*sigma_theta**2)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2ddbea98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gibbs_sampler(Y, thetas, mu_theta, a=-1/2, b=0, iterations=1000,thin=500):\n",
    "    mat = np.zeros((iterations,3))\n",
    "    n = Y.shape[1]\n",
    "    q = Y.shape[0]\n",
    "    M = q*n\n",
    "    for i in range(iterations):\n",
    "        # updates sigmas\n",
    "        s_theta = np.sqrt(abs(invgauss.rvs( q/2 + a, 1/2*np.sum(thetas - mu_theta) ) ))\n",
    "        s_epsilon = np.sqrt(abs(invgauss.rvs(M/2 + b, 1/2*np.sum(Y - thetas) ) ))\n",
    "        # updates mu\n",
    "        t = get_t(q, M, s_epsilon, s_theta)\n",
    "        Y_i = get_mean_Y(Y)\n",
    "        mu_theta = 1/t*np.sum( M*Y_i/(s_epsilon**2 + M*s_theta**2) )\n",
    "        #mu_theta = np.mean(thetas)\n",
    "        #thetas, eps = generate_data(q, n, mu_theta, s_theta, s_epsilon)\n",
    "        #Y = thetas + eps\n",
    "        mat[i] = [mu_theta, s_theta, s_epsilon]\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ca4291e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def block_gibbs_sampler(q=13, n=3, mu_theta=4.8, sigma_theta=1/2, sigma_epsilon=1/2, a=-1/2, b=0, iterations = 1000):\n",
    "    \n",
    "    thetas, epsilon = generate_data(q, n, mu_theta, sigma_theta, sigma_epsilon)\n",
    "    Y_original = thetas + epsilon\n",
    "    # gibbs sampler\n",
    "    estimators = gibbs_sampler(Y_original, thetas, mu_theta, a=-1/2, b=0, iterations=iterations)\n",
    "    best = np.mean(np.square(estimators - np.array([mu_theta, sigma_theta, sigma_epsilon])), axis=1)\n",
    "    return estimators[np.where(best == best.min())][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178e2d18",
   "metadata": {},
   "source": [
    "For data configuration, we define the following varaibles:  \n",
    "__note__ that with that configurations, the Block Gibbs Sampler should be __geometrically ergodic__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1332cb94",
   "metadata": {},
   "source": [
    "* Assess convergence (or lack thereof) and mixing of the resulting chain.\n",
    "\n",
    "__sol__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c21351ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_theta_new, sigma_theta_new, sigma_epsilon_new = block_gibbs_sampler()\n",
    "thetas_generated, epsilon_generated = generate_data(q, n, mu_theta_new, sigma_theta_new, sigma_epsilon_new)\n",
    "Y_generated = thetas_generated + epsilon_generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fe7ac070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.664951291550561, 4.673353982411732)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(Y_generated), np.mean(thetas_generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e2b853",
   "metadata": {},
   "source": [
    "$SST$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4f2d900c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.868326920244009"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n*np.sum(np.square(get_mean_Y(Y) - np.mean(Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05326ce5",
   "metadata": {},
   "source": [
    "$SSE$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "49e274d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.666098300540602"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_ = []\n",
    "for i in range(Y.shape[0]):\n",
    "    sum_.append(n*np.sum( np.square(Y[i,:] - Y_i[i]) ) )\n",
    "np.sum(sum_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aebb3d3",
   "metadata": {},
   "source": [
    "* Confirm your results agree with those given by the original authors up to Monte Carlo error.\n",
    "\n",
    "__sol__:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0b434c",
   "metadata": {},
   "source": [
    "Our new estimator $\\tilde{g}_{R}$ derivated from $\\bar{g}_N$ is defiend as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\bar{g}_{N} = \\frac{1}{N} \\sum^{N-1}_{i=0} g(X_n) = \\frac{1}{N} \\sum^{N-1}_{i=0} g(\\sigma^{2}_n, \\xi_n)\n",
    "\\end{equation}\n",
    "\n",
    "Becomes:\n",
    "\n",
    "\\begin{equation}\n",
    "\\tilde{g}_{R} = \\frac{\\sum^R_{t=1} S_t}{\\sum^R_{t=1} N_t}\n",
    "\\end{equation}\n",
    "\n",
    "Where:\n",
    "\n",
    "\\begin{equation}\n",
    "S_t = \\frac{\\sum^R_{t=1} S_t}{\\sum^R_{t=1} N_t}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b855fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "109e4d3b",
   "metadata": {},
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febcd722",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4fa7702",
   "metadata": {},
   "source": [
    "* Comment on the significance of geometric ergodicity for the blocked Gibbs sampler proposed by Tan-2009.\n",
    "\n",
    "__sol__:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d352809",
   "metadata": {},
   "source": [
    "If an estimator is __geometrically ergodicity__ and there is $\\alpha>0$ such that $E_{\\pi}|g|^{2+\\alpha}<\\infty$. Where $E_{\\pi}g$ is estimated by using the classical Monte Carlo.  \n",
    "So, in Block Gibbs sampling for a high value N of samples and dimension $q\\geq2$ we can calculate an asymptotic standard error for $\\bar{g}_N$. This means knowing some initial conditions, such as minimal number of samples to converge and minimal dimension data. Besides, the article proof that for that conditions (such as $q\\geq4$ and $M\\geq q+3$) the block gibbs is a strongly estimator."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
