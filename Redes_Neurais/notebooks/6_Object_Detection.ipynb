{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1J__0_nuOle"
      },
      "source": [
        "# **Yolo v3 practice**\n",
        "**Objective:** Understand implementation of Yolo v3 and execute a pre-trained model on Image and Video. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "*Author: Ant√¥nio Luis (Phd student Puc-Rio)*\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi\n",
        "\n",
        "!pip install keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Te6K9-lxzEEO",
        "outputId": "72d50d9a-5c57-47d7-a3a7-1b542ba4f061"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Oct 20 18:42:19 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   48C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wX_gAA3wvup6",
        "outputId": "38839f6a-fdac-48e2-bea0-573f0c07870c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.9.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29M21idK1BDq"
      },
      "source": [
        "### Import dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6eyb6FmCNbVd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "f6b2db4b-abbc-4b57-9e27-12440f39c7b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.9.0)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-28621b2077a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install keras'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLeakyReLU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZeroPadding2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUpSampling2D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0madd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras.layers.merge'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from shutil import copy2\n",
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "!pip install keras\n",
        "from keras.layers import Conv2D, Input, BatchNormalization, LeakyReLU, ZeroPadding2D, UpSampling2D\n",
        "from keras.layers.merge import add, concatenate\n",
        "from keras.models import Model\n",
        "import struct\n",
        "import cv2\n",
        "from numpy import expand_dims\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from matplotlib import pyplot\n",
        "from matplotlib.patches import Rectangle\n",
        "from IPython.display import Image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GMmXYA2WvY6"
      },
      "source": [
        "### Verify if you have GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvFPWADwWy5Q"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Llni6gaTucBz"
      },
      "source": [
        "### Setup - Connect do Gdrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHxVFkGQbGWc"
      },
      "outputs": [],
      "source": [
        "%cd ..\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyzI8iwebOyg"
      },
      "outputs": [],
      "source": [
        "# this creates a symbolic link so that now the path /content/gdrive/My\\ Drive/ is equal to /mydrive\n",
        "!ln -s /content/gdrive/My\\ Drive/ /mydrive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbbHTqrcuheP"
      },
      "source": [
        "### Setup - working folder\n",
        "\n",
        "1.   Create folder 'yolov3' in your Drive\n",
        "2.   Download yolov3 weights (pre-trained on MS-COCO)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUY7ZwLYgWjB"
      },
      "outputs": [],
      "source": [
        "!mkdir /mydrive/yolov3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXl827zOgAUe"
      },
      "outputs": [],
      "source": [
        "cd /mydrive/yolov3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVA7XX34VtPM"
      },
      "source": [
        "### Download pre-trained weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2UfFAS7b0Ir"
      },
      "outputs": [],
      "source": [
        "# get yolov3 pretrained coco dataset weights\n",
        "!wget https://pjreddie.com/media/files/yolov3.weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eKh-99t6Yeu"
      },
      "source": [
        "# PART 1  - Yolo V3 implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvdNb5jLvLIW"
      },
      "source": [
        "### Define model functions\n",
        "\n",
        "1.   WeightReader to read weight from file\n",
        "2.   make_yolov3_model to build model\n",
        "3.   _conv_block to add conv block to model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WpgdQzNEiCw0"
      },
      "outputs": [],
      "source": [
        "class WeightReader:\n",
        "    def __init__(self, weight_file):\n",
        "        with open(weight_file, 'rb') as w_f:\n",
        "            major,    = struct.unpack('i', w_f.read(4))\n",
        "            minor,    = struct.unpack('i', w_f.read(4))\n",
        "            revision, = struct.unpack('i', w_f.read(4))\n",
        "\n",
        "            if (major*10 + minor) >= 2 and major < 1000 and minor < 1000:\n",
        "                w_f.read(8)\n",
        "            else:\n",
        "                w_f.read(4)\n",
        "\n",
        "            transpose = (major > 1000) or (minor > 1000)\n",
        "            \n",
        "            binary = w_f.read()\n",
        "\n",
        "        self.offset = 0\n",
        "        self.all_weights = np.frombuffer(binary, dtype='float32')\n",
        "        \n",
        "    def read_bytes(self, size):\n",
        "        self.offset = self.offset + size\n",
        "        return self.all_weights[self.offset-size:self.offset]\n",
        "\n",
        "    def load_weights(self, model):\n",
        "        for i in range(106):\n",
        "            try:\n",
        "                conv_layer = model.get_layer('conv_' + str(i))\n",
        "                print(\"loading weights of convolution #\" + str(i))\n",
        "\n",
        "                if i not in [81, 93, 105]:\n",
        "                    norm_layer = model.get_layer('bnorm_' + str(i))\n",
        "\n",
        "                    size = np.prod(norm_layer.get_weights()[0].shape)\n",
        "\n",
        "                    beta  = self.read_bytes(size) # bias\n",
        "                    gamma = self.read_bytes(size) # scale\n",
        "                    mean  = self.read_bytes(size) # mean\n",
        "                    var   = self.read_bytes(size) # variance            \n",
        "\n",
        "                    weights = norm_layer.set_weights([gamma, beta, mean, var])  \n",
        "\n",
        "                if len(conv_layer.get_weights()) > 1:\n",
        "                    bias   = self.read_bytes(np.prod(conv_layer.get_weights()[1].shape))\n",
        "                    kernel = self.read_bytes(np.prod(conv_layer.get_weights()[0].shape))\n",
        "                    \n",
        "                    kernel = kernel.reshape(list(reversed(conv_layer.get_weights()[0].shape)))\n",
        "                    kernel = kernel.transpose([2,3,1,0])\n",
        "                    conv_layer.set_weights([kernel, bias])\n",
        "                else:\n",
        "                    kernel = self.read_bytes(np.prod(conv_layer.get_weights()[0].shape))\n",
        "                    kernel = kernel.reshape(list(reversed(conv_layer.get_weights()[0].shape)))\n",
        "                    kernel = kernel.transpose([2,3,1,0])\n",
        "                    conv_layer.set_weights([kernel])\n",
        "            except ValueError:\n",
        "                print(\"no convolution #\" + str(i))     \n",
        "    \n",
        "    def reset(self):\n",
        "        self.offset = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9ZKkiwj4nrv"
      },
      "source": [
        "![yolov3-darknet](https://miro.medium.com/max/2000/1*d4Eg17IVJ0L41e7CTWLLSg.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yv6JDfA4hS0M"
      },
      "outputs": [],
      "source": [
        "def make_yolov3_model():\n",
        "    input_image = Input(shape=(None, None, 3))\n",
        "\n",
        "    # Layer  0 => 4\n",
        "    x = _conv_block(input_image, [{'filter': 32, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 0},\n",
        "                                  {'filter': 64, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 1},\n",
        "                                  {'filter': 32, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 2},\n",
        "                                  {'filter': 64, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 3}])\n",
        "\n",
        "    # Layer  5 => 8\n",
        "    x = _conv_block(x, [{'filter': 128, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 5},\n",
        "                        {'filter':  64, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 6},\n",
        "                        {'filter': 128, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 7}])\n",
        "\n",
        "    # Layer  9 => 11\n",
        "    x = _conv_block(x, [{'filter':  64, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 9},\n",
        "                        {'filter': 128, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 10}])\n",
        "\n",
        "    # Layer 12 => 15\n",
        "    x = _conv_block(x, [{'filter': 256, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 12},\n",
        "                        {'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 13},\n",
        "                        {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 14}])\n",
        "\n",
        "    # Layer 16 => 36\n",
        "    for i in range(7):\n",
        "        x = _conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 16+i*3},\n",
        "                            {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 17+i*3}])\n",
        "        \n",
        "    skip_36 = x\n",
        "        \n",
        "    # Layer 37 => 40\n",
        "    x = _conv_block(x, [{'filter': 512, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 37},\n",
        "                        {'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 38},\n",
        "                        {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 39}])\n",
        "\n",
        "    # Layer 41 => 61\n",
        "    for i in range(7):\n",
        "        x = _conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 41+i*3},\n",
        "                            {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 42+i*3}])\n",
        "        \n",
        "    skip_61 = x\n",
        "        \n",
        "    # Layer 62 => 65\n",
        "    x = _conv_block(x, [{'filter': 1024, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 62},\n",
        "                        {'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 63},\n",
        "                        {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 64}])\n",
        "\n",
        "    # Layer 66 => 74\n",
        "    for i in range(3):\n",
        "        x = _conv_block(x, [{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 66+i*3},\n",
        "                            {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 67+i*3}])\n",
        "        \n",
        "    # Layer 75 => 79\n",
        "    x = _conv_block(x, [{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 75},\n",
        "                        {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 76},\n",
        "                        {'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 77},\n",
        "                        {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 78},\n",
        "                        {'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 79}], skip=False)\n",
        "\n",
        "    # Layer 80 => 82\n",
        "    yolo_82 = _conv_block(x, [{'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 80},\n",
        "                              {'filter':  255, 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 81}], skip=False)\n",
        "\n",
        "    # Layer 83 => 86\n",
        "    x = _conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 84}], skip=False)\n",
        "    x = UpSampling2D(2)(x)\n",
        "    x = concatenate([x, skip_61])\n",
        "\n",
        "    # Layer 87 => 91\n",
        "    x = _conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 87},\n",
        "                        {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 88},\n",
        "                        {'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 89},\n",
        "                        {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 90},\n",
        "                        {'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 91}], skip=False)\n",
        "\n",
        "    # Layer 92 => 94\n",
        "    yolo_94 = _conv_block(x, [{'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 92},\n",
        "                              {'filter': 255, 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 93}], skip=False)\n",
        "\n",
        "    # Layer 95 => 98\n",
        "    x = _conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True,   'layer_idx': 96}], skip=False)\n",
        "    x = UpSampling2D(2)(x)\n",
        "    x = concatenate([x, skip_36])\n",
        "\n",
        "    # Layer 99 => 106\n",
        "    yolo_106 = _conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 99},\n",
        "                               {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 100},\n",
        "                               {'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 101},\n",
        "                               {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 102},\n",
        "                               {'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 103},\n",
        "                               {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 104},\n",
        "                               {'filter': 255, 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 105}], skip=False)\n",
        "\n",
        "    model = Model(input_image, [yolo_82, yolo_94, yolo_106])    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbsbipWXhWpN"
      },
      "outputs": [],
      "source": [
        "def _conv_block(inp, convs, skip=True):\n",
        "    x = inp\n",
        "    count = 0\n",
        "    \n",
        "    for conv in convs:\n",
        "        if count == (len(convs) - 2) and skip:\n",
        "            skip_connection = x\n",
        "        count += 1\n",
        "        \n",
        "        if conv['stride'] > 1: x = ZeroPadding2D(((1,0),(1,0)))(x) # peculiar padding as darknet prefer left and top\n",
        "        x = Conv2D(conv['filter'], \n",
        "                   conv['kernel'], \n",
        "                   strides=conv['stride'], \n",
        "                   padding='valid' if conv['stride'] > 1 else 'same', # peculiar padding as darknet prefer left and top\n",
        "                   name='conv_' + str(conv['layer_idx']), \n",
        "                   use_bias=False if conv['bnorm'] else True)(x)\n",
        "        if conv['bnorm']: x = BatchNormalization(epsilon=0.001, name='bnorm_' + str(conv['layer_idx']))(x)\n",
        "        if conv['leaky']: x = LeakyReLU(alpha=0.1, name='leaky_' + str(conv['layer_idx']))(x)\n",
        "\n",
        "    return add([skip_connection, x]) if skip else x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wyl0ylOlwIMD"
      },
      "source": [
        "### Define and save model with pre-trained weights\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kohx_yY1hoDx"
      },
      "outputs": [],
      "source": [
        "# define the model\n",
        "model = make_yolov3_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggJqdLhuhp56"
      },
      "outputs": [],
      "source": [
        "# load the model weights\n",
        "weight_reader = WeightReader('/mydrive/yolov3/yolov3.weights') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtIBObcyiHwF"
      },
      "outputs": [],
      "source": [
        "# set the model weights into the model\n",
        "weight_reader.load_weights(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZwJPg6HiMWq"
      },
      "outputs": [],
      "source": [
        "# save the model to file\n",
        "model.save('model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7kJd6Q7wTJ6"
      },
      "source": [
        "## Load and Execute model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQzd7MkXiQR6"
      },
      "outputs": [],
      "source": [
        "# load yolov3 model\n",
        "model = load_model('model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwIlFrwvwkcv"
      },
      "source": [
        "### Download test image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bG8fIf7leX5F"
      },
      "outputs": [],
      "source": [
        "# download test image\n",
        "!wget https://www.dropbox.com/s/i0j17woc0s90xve/TRAFFIC.jpg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YF10TFqawxWm"
      },
      "outputs": [],
      "source": [
        "# download test image\n",
        "!wget https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2019/03/zebra.jpg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-q3TI__12-6"
      },
      "outputs": [],
      "source": [
        "Image('zebra.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sICuWVOyelAa"
      },
      "outputs": [],
      "source": [
        "Image('TRAFFIC.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nz2CCKp7kzY1"
      },
      "outputs": [],
      "source": [
        "# load and prepare an image\n",
        "def load_image_pixels(filename, shape):\n",
        "  '''\n",
        "  Load image, reshape to intended shape (416x416 for yolo)\n",
        "  and normalize images to pixel values in 0-1.\n",
        "  Save original width and height of image to reshape back to\n",
        "  original size the picture with bounding boxes later.\n",
        "  '''\n",
        "  # load the image to get its shape\n",
        "  image = load_img(filename)\n",
        "  width, height = image.size\n",
        "  # load the image with the required size\n",
        "  image = load_img(filename, target_size=shape)\n",
        "  # convert to numpy array\n",
        "  image = img_to_array(image)\n",
        "  # scale pixel values to [0, 1]\n",
        "  image = image.astype('float32')\n",
        "  image /= 255.0\n",
        "  # add a dimension so that we have one sample\n",
        "  image = expand_dims(image, 0)\n",
        "  return image, width, height"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_SvkdeKmaCm"
      },
      "outputs": [],
      "source": [
        "\n",
        "# define the expected input shape for the model\n",
        "input_w, input_h = 416, 416\n",
        "\n",
        "#CHANGE CODE HERE\n",
        "##############################################################\n",
        "# define our new photo\n",
        "photo_filename = 'zebra.jpg' #'TRAFFIC.jpg'      ######################## <-----  EXERCISE CHANGE HERE \n",
        "##############################################################\n",
        "# load and prepare image\n",
        "image, image_w, image_h = load_image_pixels(photo_filename, (input_w, input_h))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjOUZsHYtt7d"
      },
      "source": [
        "### Make prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pr2VFI0WCAKW"
      },
      "source": [
        "![alt text](https://miro.medium.com/max/1200/0*3A8U0Hm5IKmRa6hu.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGFwXnCUmjRV"
      },
      "outputs": [],
      "source": [
        "# make prediction\n",
        "yhat = model.predict(image)\n",
        "# summarize the shape of the list of arrays\n",
        "print([a.shape for a in yhat])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esyoAIpL6aD6"
      },
      "source": [
        "Note that the shape of the detection kernel is 1 x 1 x (B x (5 + C) ). Here B is the number of bounding boxes a cell on the feature map can predict, ‚Äú5‚Äù is for the 4 bounding box attributes and one object confidence, and C is the number of classes. In YOLO v3 trained on COCO, B = 3 and C = 80, so the kernel size is 1 x 1 x (3x85) = 1 x 1 x 255\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcrhnbCHyJWR"
      },
      "source": [
        "### Extract and select bounding boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LN2-8Z0SqcUt"
      },
      "outputs": [],
      "source": [
        "class BoundBox:\n",
        "\tdef __init__(self, xmin, ymin, xmax, ymax, objness = None, classes = None):\n",
        "\t\tself.xmin = xmin\n",
        "\t\tself.ymin = ymin\n",
        "\t\tself.xmax = xmax\n",
        "\t\tself.ymax = ymax\n",
        "\t\tself.objness = objness\n",
        "\t\tself.classes = classes\n",
        "\t\tself.label = -1\n",
        "\t\tself.score = -1\n",
        " \n",
        "\tdef get_label(self):\n",
        "\t\tif self.label == -1:\n",
        "\t\t\tself.label = np.argmax(self.classes)\n",
        " \n",
        "\t\treturn self.label\n",
        " \n",
        "\tdef get_score(self):\n",
        "\t\tif self.score == -1:\n",
        "\t\t\tself.score = self.classes[self.get_label()]\n",
        " \n",
        "\t\treturn self.score\n",
        " \n",
        "def _sigmoid(x):\n",
        "\treturn 1. / (1. + np.exp(-x))\n",
        " \n",
        "def decode_netout(netout, anchors, obj_thresh, net_h, net_w):\n",
        "\tgrid_h, grid_w = netout.shape[:2]\n",
        "\tnb_box = 3\n",
        "\tnetout = netout.reshape((grid_h, grid_w, nb_box, -1))\n",
        "\tnb_class = netout.shape[-1] - 5\n",
        "\tboxes = []\n",
        "\tnetout[..., :2]  = _sigmoid(netout[..., :2])\n",
        "\tnetout[..., 4:]  = _sigmoid(netout[..., 4:])\n",
        "\tnetout[..., 5:]  = netout[..., 4][..., np.newaxis] * netout[..., 5:]\n",
        "\tnetout[..., 5:] *= netout[..., 5:] > obj_thresh\n",
        " \n",
        "\tfor i in range(grid_h*grid_w):\n",
        "\t\trow = i / grid_w\n",
        "\t\tcol = i % grid_w\n",
        "\t\tfor b in range(nb_box):\n",
        "\t\t\t# 4th element is objectness score\n",
        "\t\t\tobjectness = netout[int(row)][int(col)][b][4]\n",
        "\t\t\tif(objectness.all() <= obj_thresh): continue\n",
        "\t\t\t# first 4 elements are x, y, w, and h\n",
        "\t\t\tx, y, w, h = netout[int(row)][int(col)][b][:4]\n",
        "\t\t\tx = (col + x) / grid_w # center position, unit: image width\n",
        "\t\t\ty = (row + y) / grid_h # center position, unit: image height\n",
        "\t\t\tw = anchors[2 * b + 0] * np.exp(w) / net_w # unit: image width\n",
        "\t\t\th = anchors[2 * b + 1] * np.exp(h) / net_h # unit: image height\n",
        "\t\t\t# last elements are class probabilities\n",
        "\t\t\tclasses = netout[int(row)][col][b][5:]\n",
        "\t\t\tbox = BoundBox(x-w/2, y-h/2, x+w/2, y+h/2, objectness, classes)\n",
        "\t\t\tboxes.append(box)\n",
        "\treturn boxes\n",
        " \n",
        "def correct_yolo_boxes(boxes, image_h, image_w, net_h, net_w):\n",
        "\tnew_w, new_h = net_w, net_h\n",
        "\tfor i in range(len(boxes)):\n",
        "\t\tx_offset, x_scale = (net_w - new_w)/2./net_w, float(new_w)/net_w\n",
        "\t\ty_offset, y_scale = (net_h - new_h)/2./net_h, float(new_h)/net_h\n",
        "\t\tboxes[i].xmin = int((boxes[i].xmin - x_offset) / x_scale * image_w)\n",
        "\t\tboxes[i].xmax = int((boxes[i].xmax - x_offset) / x_scale * image_w)\n",
        "\t\tboxes[i].ymin = int((boxes[i].ymin - y_offset) / y_scale * image_h)\n",
        "\t\tboxes[i].ymax = int((boxes[i].ymax - y_offset) / y_scale * image_h)\n",
        " \n",
        "def _interval_overlap(interval_a, interval_b):\n",
        "\tx1, x2 = interval_a\n",
        "\tx3, x4 = interval_b\n",
        "\tif x3 < x1:\n",
        "\t\tif x4 < x1:\n",
        "\t\t\treturn 0\n",
        "\t\telse:\n",
        "\t\t\treturn min(x2,x4) - x1\n",
        "\telse:\n",
        "\t\tif x2 < x3:\n",
        "\t\t\t return 0\n",
        "\t\telse:\n",
        "\t\t\treturn min(x2,x4) - x3\n",
        " \n",
        "def bbox_iou(box1, box2):\n",
        "\tintersect_w = _interval_overlap([box1.xmin, box1.xmax], [box2.xmin, box2.xmax])\n",
        "\tintersect_h = _interval_overlap([box1.ymin, box1.ymax], [box2.ymin, box2.ymax])\n",
        "\tintersect = intersect_w * intersect_h\n",
        "\tw1, h1 = box1.xmax-box1.xmin, box1.ymax-box1.ymin\n",
        "\tw2, h2 = box2.xmax-box2.xmin, box2.ymax-box2.ymin\n",
        "\tunion = w1*h1 + w2*h2 - intersect\n",
        "\treturn float(intersect) / union\n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HNmaNesDGpK"
      },
      "source": [
        "![alt text](https://miro.medium.com/max/1400/1*mEIEF1xvFAWHJeJWxnGLaw.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gv5tl3NjxSnz"
      },
      "source": [
        "### Do Non-maximal supression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0bJj8kFCcQh"
      },
      "source": [
        "![alt text](https://miro.medium.com/max/1400/1*6d_D0ySg-kOvfrzIRwHIiA.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1r353XrcxKGX"
      },
      "outputs": [],
      "source": [
        "def do_nms(boxes, nms_thresh):\n",
        "\tif len(boxes) > 0:\n",
        "\t\tnb_class = len(boxes[0].classes)\n",
        "\telse:\n",
        "\t\treturn\n",
        "\tfor c in range(nb_class):\n",
        "\t\tsorted_indices = np.argsort([-box.classes[c] for box in boxes])\n",
        "\t\tfor i in range(len(sorted_indices)):\n",
        "\t\t\tindex_i = sorted_indices[i]\n",
        "\t\t\tif boxes[index_i].classes[c] == 0: continue\n",
        "\t\t\tfor j in range(i+1, len(sorted_indices)):\n",
        "\t\t\t\tindex_j = sorted_indices[j]\n",
        "\t\t\t\tif bbox_iou(boxes[index_i], boxes[index_j]) >= nms_thresh:\n",
        "\t\t\t\t\tboxes[index_j].classes[c] = 0\n",
        " \n",
        " \n",
        "# get all of the results above a threshold\n",
        "def get_boxes(boxes, labels, thresh):\n",
        "\tv_boxes, v_labels, v_scores = list(), list(), list()\n",
        "\t# enumerate all boxes\n",
        "\tfor box in boxes:\n",
        "\t\t# enumerate all possible labels\n",
        "\t\tfor i in range(len(labels)):\n",
        "\t\t\t# check if the threshold for this label is high enough\n",
        "\t\t\tif box.classes[i] > thresh:\n",
        "\t\t\t\tv_boxes.append(box)\n",
        "\t\t\t\tv_labels.append(labels[i])\n",
        "\t\t\t\tv_scores.append(box.classes[i]*100)\n",
        "\t\t\t\t# don't break, many labels may trigger for one box\n",
        "\treturn v_boxes, v_labels, v_scores\n",
        " \n",
        "# draw all results\n",
        "def draw_boxes(filename, v_boxes, v_labels, v_scores):\n",
        "\t# load the image\n",
        "\tdata = pyplot.imread(filename)\n",
        "\t# plot the image\n",
        "\tpyplot.imshow(data)\n",
        "\t# get the context for drawing boxes\n",
        "\tax = pyplot.gca()\n",
        "\t# plot each box\n",
        "\tfor i in range(len(v_boxes)):\n",
        "\t\tbox = v_boxes[i]\n",
        "\t\t# get coordinates\n",
        "\t\ty1, x1, y2, x2 = box.ymin, box.xmin, box.ymax, box.xmax\n",
        "\t\t# calculate width and height of the box\n",
        "\t\twidth, height = x2 - x1, y2 - y1\n",
        "\t\t# create the shape\n",
        "\t\trect = Rectangle((x1, y1), width, height, fill=False, color='white')\n",
        "\t\t# draw the box\n",
        "\t\tax.add_patch(rect)\n",
        "\t\t# draw text and score in top left corner\n",
        "\t\tlabel = \"%s (%.3f)\" % (v_labels[i], v_scores[i])\n",
        "\t\tpyplot.text(x1, y1, label, color='white')\n",
        "\t# show the plot\n",
        "\t#pyplot.savefig('figura_zebra.png')\n",
        "\tpyplot.show()\n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jst1T1KCnZX"
      },
      "source": [
        "### Define config parameters and run the model *****\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpniYnLGxpeI"
      },
      "outputs": [],
      "source": [
        "# define the anchors\n",
        "anchors = [[116,90, 156,198, 373,326], [30,61, 62,45, 59,119], [10,13, 16,30, 33,23]]\n",
        "#CHANGE CODE HERE\n",
        "########################################################################\n",
        "# define the probability threshold for detected objects\n",
        "class_threshold = 0.6\n",
        "nms_threshold = 0.5\n",
        "#############################################################################\n",
        "boxes = list()\n",
        "pyplot.rcParams[\"figure.figsize\"] = (40,10)\n",
        "for i in range(len(yhat)):\n",
        "\t# decode the output of the network\n",
        "\tboxes += decode_netout(yhat[i][0], anchors[i], class_threshold, input_h, input_w)\n",
        "# correct the sizes of the bounding boxes for the shape of the image\n",
        "correct_yolo_boxes(boxes, image_h, image_w, input_h, input_w)\n",
        "# suppress non-maximal boxes\n",
        "do_nms(boxes, nms_threshold)\n",
        "# define the labels\n",
        "labels = [\"person\", \"bicycle\", \"car\", \"motorbike\", \"aeroplane\", \"bus\", \"train\", \"truck\",\n",
        "\t\"boat\", \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\",\n",
        "\t\"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\",\n",
        "\t\"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\",\n",
        "\t\"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\",\n",
        "\t\"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\",\n",
        "\t\"apple\", \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\",\n",
        "\t\"chair\", \"sofa\", \"pottedplant\", \"bed\", \"diningtable\", \"toilet\", \"tvmonitor\", \"laptop\", \"mouse\",\n",
        "\t\"remote\", \"keyboard\", \"cell phone\", \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\",\n",
        "\t\"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\", \"hair drier\", \"toothbrush\"]\n",
        "# get the details of the detected objects\n",
        "v_boxes, v_labels, v_scores = get_boxes(boxes, labels, class_threshold)\n",
        "# summarize what we found\n",
        "for i in range(len(v_boxes)):\n",
        "\tprint(v_labels[i], v_scores[i])\n",
        "# draw what we found\n",
        "draw_boxes(photo_filename, v_boxes, v_labels, v_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_GvsL95Z3_O"
      },
      "outputs": [],
      "source": [
        "print(\"Number of objects detected in the scene:\" ,len(v_boxes))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FV5p9xSYvOPe"
      },
      "source": [
        "# **PART 2: *Yolo* v3 - on Video (in Pytorch)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgr6U8m0A8dk"
      },
      "source": [
        "On this part we are going to use a different implementation, a Pytorch implementation from Ayoosh Kathuria"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hR0ny5FdLTGZ"
      },
      "source": [
        "Example:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-8lrnFoBK-X"
      },
      "source": [
        "Clone the repo we are going to use "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSoKFdocvNS0"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/ayooshkathuria/pytorch-yolo-v3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDAtqYkELujD"
      },
      "source": [
        "Enter the repo we just cloned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDf2vzqHzsHD"
      },
      "outputs": [],
      "source": [
        "cd pytorch-yolo-v3/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peVrGV_N2VbM"
      },
      "source": [
        "### Import dependencies (we use many from the repo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JoKEA1fZ2TsE"
      },
      "outputs": [],
      "source": [
        "from __future__ import division\n",
        "import time\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from util import *\n",
        "from darknet import Darknet\n",
        "from preprocess import prep_image, inp_to_image, letterbox_image\n",
        "import random \n",
        "import pickle as pkl\n",
        "from google.colab.patches import cv2_imshow #cv2.imshow causes collab to crash\n",
        "                                            # then we use cv2_imshow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VndRcbP2Ag11"
      },
      "source": [
        "**Download or Upload** your own video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vlz4f2czx_oS"
      },
      "outputs": [],
      "source": [
        "#Upload video on yolov3 folder on your Google Drive\n",
        "\n",
        "#CHANGE CODE HERE\n",
        "#########################################################\n",
        "video_name = \"chasing_animals360p.mp4\"\n",
        "\n",
        "# Just uncomment the 3 lines below\n",
        "\n",
        "#orig_folder = \"/mydrive/yolov3/\" + video_name\n",
        "#dest_folder = \"/content/pytorch-yolo-v3\"\n",
        "#copy2(orig_folder,dest_folder)\n",
        "##########################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xZvt6DdPdGK"
      },
      "outputs": [],
      "source": [
        "# Or just download from a link right into your pytorch-yolo-v3 folder\n",
        "\n",
        "#CHANGE CODE HERE\n",
        "######################################\n",
        "#Uncomment and \n",
        "!wget \"https://www.dropbox.com/s/951sqbtge8dzcsr/chasing_animals360p.mp4\"\n",
        "#######################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZQSG2wyx6w5"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "video_path = video_name ######## put the name of your video file HERE\n",
        "\n",
        "mp4 = open(video_path,'rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "\n",
        "HTML(\"\"\"\n",
        "<video width=400 controls>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Qe4TowTB_Fh"
      },
      "source": [
        "Auxiliary functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kfkFnq51wKy"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_test_input(input_dim, CUDA):\n",
        "    img = cv2.imread(\"dog-cycle-car.png\")\n",
        "    img = cv2.resize(img, (input_dim, input_dim)) \n",
        "    img_ =  img[:,:,::-1].transpose((2,0,1))\n",
        "    img_ = img_[np.newaxis,:,:,:]/255.0\n",
        "    img_ = torch.from_numpy(img_).float()\n",
        "    img_ = Variable(img_)\n",
        "    \n",
        "    if CUDA:\n",
        "        img_ = img_.cuda()\n",
        "    \n",
        "    return img_\n",
        "\n",
        "def prep_image(img, inp_dim):\n",
        "    \"\"\"\n",
        "    Prepare image for inputting to the neural network. \n",
        "    \n",
        "    Returns a Variable \n",
        "    \"\"\"\n",
        "\n",
        "    orig_im = img\n",
        "    dim = orig_im.shape[1], orig_im.shape[0]\n",
        "    img = (letterbox_image(orig_im, (inp_dim, inp_dim)))\n",
        "    img_ = img[:,:,::-1].transpose((2,0,1)).copy()\n",
        "    img_ = torch.from_numpy(img_).float().div(255.0).unsqueeze(0)\n",
        "    return img_, orig_im, dim\n",
        "\n",
        "def write(x, img):\n",
        "    c1 = tuple(x[1:3].int())\n",
        "    c2 = tuple(x[3:5].int())\n",
        "    cls = int(x[-1]) if x[-1] < len(classes) else -1\n",
        "    label = \"{0}\".format(classes[cls]) if cls != -1 else 'no detection'\n",
        "    if label != 'no detection':\n",
        "      color = random.choice(colors)\n",
        "      cv2.rectangle(img, (int(c1[0]), int(c1[1])), (int(c2[0]), int(c2[1])),color, 1)\n",
        "      t_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_PLAIN, 1 , 1)[0]\n",
        "      c2 = c1[0] + t_size[0] + 3, c1[1] + t_size[1] + 4\n",
        "      cv2.rectangle(img, (int(c1[0]), int(c1[1])), (int(c2[0]), int(c2[1])),color, -1)\n",
        "      cv2.putText(img, label, (int(c1[0]), int(c1[1] + t_size[1] + 4)), cv2.FONT_HERSHEY_PLAIN, 1, [225,255,255], 1);\n",
        "    return img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vlE4JiMCFdI"
      },
      "source": [
        "Defining setup parameters for YOLO v3 - Video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8M1yGIqcB5tJ"
      },
      "outputs": [],
      "source": [
        "# Input arguments\n",
        "\n",
        "#CHANGE CODE HERE\n",
        "####################################\n",
        "video = video_name     ######## put the name of your video file HERE\n",
        "\n",
        "confidence = 0.6 # Object Confidence to filter predictions - float32\n",
        "\n",
        "nms_thresh = 0.4 # Non-maximal supression threshold - float32\n",
        "\n",
        "save_fps = 30. # Important to save the processed video with the right fps\n",
        "####################################\n",
        "\n",
        "dataset = \"coco\" # \"Dataset on which the network has been trained\"\n",
        "cfgfile = \"cfg/yolov3.cfg\" # Config file\n",
        "\n",
        "weightsfile = \"/mydrive/yolov3/yolov3.weights\" # pre-trained weights \n",
        "\n",
        "reso = \"416\" #Input resolution of the network. Increase to increase accuracy.\n",
        "             #Decrease to increase speed\n",
        "\n",
        "\n",
        "# Video save codec \n",
        "##change this for different video types if you get errors\n",
        "fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
        "#Other options\n",
        "# cv2.VideoWriter_fourcc(*'MP4V')\n",
        "#cv2.VideoWriter_fourcc('M','P','E','G')\n",
        "#cv2.VideoWriter_fourcc('M', 'J', 'P', 'G') \n",
        "#cv2.VideoWriter_fourcc(*'DIVX') -> para .avi\n",
        "# cv2.VideoWriter_fourcc(*'MP4V')             "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SD0W8NLICZrA"
      },
      "source": [
        "### Run video prediction and save video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EylRU9Nq4mrU"
      },
      "outputs": [],
      "source": [
        "start = 0\n",
        "\n",
        "CUDA = torch.cuda.is_available()\n",
        "\n",
        "num_classes = 80\n",
        "\n",
        "CUDA = torch.cuda.is_available()\n",
        "\n",
        "bbox_attrs = 5 + num_classes\n",
        "\n",
        "print(\"Loading network.....\")\n",
        "model = Darknet(cfgfile)\n",
        "model.load_weights(weightsfile)\n",
        "print(\"Network successfully loaded\")\n",
        "\n",
        "model.net_info[\"height\"] = reso\n",
        "inp_dim = int(model.net_info[\"height\"])\n",
        "assert inp_dim % 32 == 0 \n",
        "assert inp_dim > 32\n",
        "\n",
        "if CUDA:\n",
        "    model.cuda()\n",
        "    \n",
        "model(get_test_input(inp_dim, CUDA), CUDA)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "videofile = video\n",
        "\n",
        "\n",
        "cap = cv2.VideoCapture(videofile)\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "print(\"Video resolution : \", height, width)\n",
        "\n",
        "assert cap.isOpened(), 'Cannot capture source'\n",
        "\n",
        "#Video save\n",
        "video_yolo = video[:-4] + \"_yolov3.mp4\" #[:-4] is to remove extension part of video name\n",
        "videoSaved = cv2.VideoWriter(video_yolo, apiPreference=cv2.CAP_FFMPEG,\n",
        "                             fourcc = fourcc,fps =save_fps, frameSize=(width, height),\t\n",
        "                             isColor= True)\n",
        "\n",
        "frames = 0\n",
        "start = time.time()    \n",
        "while cap.isOpened():\n",
        "    \n",
        "    ret, frame = cap.read()\n",
        "    if ret:\n",
        "        \n",
        "\n",
        "        img, orig_im, dim = prep_image(frame, inp_dim)\n",
        "        \n",
        "        im_dim = torch.FloatTensor(dim).repeat(1,2)                        \n",
        "        \n",
        "        \n",
        "        if CUDA:\n",
        "            im_dim = im_dim.cuda()\n",
        "            img = img.cuda()\n",
        "        \n",
        "        with torch.no_grad():   \n",
        "            output = model(Variable(img), CUDA)\n",
        "        output = write_results(output, confidence, num_classes, nms = True, nms_conf = nms_thresh)\n",
        "\n",
        "        if type(output) == int:\n",
        "            frames += 1\n",
        "            print(\"FPS of the video is {:5.2f}\".format( frames / (time.time() - start)))\n",
        "            cv2_imshow(\"frame\", orig_im)\n",
        "            key = cv2.waitKey(1)\n",
        "            if key & 0xFF == ord('q'):\n",
        "                break\n",
        "            continue\n",
        "\n",
        "        \n",
        "        im_dim = im_dim.repeat(output.size(0), 1)\n",
        "        scaling_factor = torch.min(inp_dim/im_dim,1)[0].view(-1,1)\n",
        "        \n",
        "        output[:,[1,3]] -= (inp_dim - scaling_factor*im_dim[:,0].view(-1,1))/2\n",
        "        output[:,[2,4]] -= (inp_dim - scaling_factor*im_dim[:,1].view(-1,1))/2\n",
        "        \n",
        "        output[:,1:5] /= scaling_factor\n",
        "\n",
        "        for i in range(output.shape[0]):\n",
        "            output[i, [1,3]] = torch.clamp(output[i, [1,3]], 0.0, im_dim[i,0])\n",
        "            output[i, [2,4]] = torch.clamp(output[i, [2,4]], 0.0, im_dim[i,1])\n",
        "        \n",
        "        classes = load_classes('data/coco.names')\n",
        "        colors = pkl.load(open(\"pallete\", \"rb\"))\n",
        "        \n",
        "        list(map(lambda x: write(x, orig_im), output))\n",
        "        \n",
        "        # video save line\n",
        "        videoSaved.write(orig_im)\n",
        "        \n",
        "        #################TO PLOT FRAMES IN REAL TIME UNCOMMENT LINE BELOW#########\n",
        "        \n",
        "        #cv2_imshow(orig_im)     # Uncomment at your own risk!!! - May freeze colab\n",
        "        if frames%100 == 0:\n",
        "          print('{} frames processed'.format(frames))\n",
        "\n",
        "        frames += 1\n",
        "        \n",
        "        ########WHEN DISPLAYNG FRAMES IN REAL TIME UNCOMMENT LINE BELOW TO PRINT FPS#####\n",
        "        \n",
        "        #print(\"FPS of the video is {:5.2f}\".format( frames / (time.time() - start)))\n",
        "\n",
        "        \n",
        "    else:\n",
        "        break\n",
        "videoSaved.release()\n",
        "end = time.time()\n",
        "print(\"Video processed with YOLO v3 saved with as \" + video_yolo)\n",
        "print('Total time elapsed: {:.3f} segundos'.format((end - start)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ratrrO_rJpL3"
      },
      "source": [
        "## Now open your Google Drive and play your processed video!\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pkif_bfXtBvn"
      },
      "source": [
        "# Exercise \n",
        "\n",
        "Apply Yolo v3 on videos and change hyperparameters (**confidence_threshold** and\n",
        "**nms_threshold**) to see the results.\n",
        "\n",
        "[Link to report the results](https://docs.google.com/document/d/1C6VzzsZnC5rm0ThX0un-0ASTwSphYJ1GKL8ssAJyoL4/edit?usp=sharing)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lzrGbBpftRq"
      },
      "source": [
        "List of videos to apply Yolo v3\n",
        "\n",
        "\n",
        "1. Indian War (30s) - 1080p : [original link](https://www.youtube.com/watch?v=DcV7d3py-Mw) /[download link](https://www.dropbox.com/s/mmncfyccvgevm87/IndianWar1080p.mp4)\n",
        "\t\t\t\t\t       \t\t\t\t\n",
        "                        \n",
        "2. Soccer tactics (2min) 360p \t: [original link](https://www.youtube.com/watch?v=c83yE-s_Wf0) /[download link](https://www.dropbox.com/s/lf99d8lzwqlj2zs/soccer_tactics360p.mp4)\n",
        "\n",
        "3. Girl soccer fight (17s) - 720p : [original link](https://www.youtube.com/watch?v=oWJLump8Jjk/)/ [download link](https://www.dropbox.com/s/0o1bourc6oyil5a/soccer_fight720p.mp4)\n",
        "                \n",
        "\n",
        "4. Girl soccer fight (17s) - 360p:\t [original link](https://www.youtube.com/watch?v=oWJLump8Jjk/)/ [download link](https://www.dropbox.com/s/35jw9yy0hle0wbw/soccer_fight360p.mp4)\t\t\t\n",
        "\t\t\n",
        "5. Animals chasing people (2min 25s) 360p :  [original link](https://www.youtube.com/watch?v=F1svRmDlsL4)/ [download link]( https://www.dropbox.com/s/951sqbtge8dzcsr/chasing_animals360p.mp4)\t     \n",
        "\n",
        "6. Hungry monkeys (40s) 720p:\t\t\t\t [original link](https://www.youtube.com/watch?v=22JgHBb-0dg)/ [download link](https://www.dropbox.com/s/isxbju7yabzlnin/hungry_monkeys720p.mp4)  \n",
        "\t\t\t\t\t\t\t\t\t\n",
        "7. Super animals (30s) 720p:\t\t [original link]( https://www.youtube.com/watch?v=PaMPdz-3Agg)/ [download link](https://www.dropbox.com/s/cuku94guronf5tf/super_animals720p.mp4) \t\n",
        "\n",
        "8. Heavy Traffic Stock Video (9s) 720p: [original link](https://www.youtube.com/watch?v=5YbLr0HRCiw&ab_channel=MotionArray) / [download link](https://www.dropbox.com/s/db4wnplsq1n420s/Heavy_Traffic_Stock_Video.mp4)\n",
        "\n",
        "9. Indian Traffic (3min 28s) 720p: [original link](https://www.youtube.com/watch?v=KnPiP9PkLAs) / [download link](https://www.dropbox.com/s/25hts0d0di9uaw2/indian_traffic.mp4)\n",
        "\n",
        "OBS: Right click and choose \"stats for nerds\"  to see frame rate of video you want to download (normally its 25~30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfxyuhCbrwcb"
      },
      "outputs": [],
      "source": [
        "#Obs: Download the video inside the \"pytorch-yolo-v3\" folder using the command wget as shown below:\n",
        "!wget \"https://www.dropbox.com/s/lf99d8lzwqlj2zs/soccer_tactics360p.mp4\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pT6AtbLRttFG"
      },
      "source": [
        "# References\n",
        "---\n",
        "\n",
        "Yolo v3 for Images practice was based on: \n",
        "\n",
        ">  [Machine Learning Mastery Tutorial](https://https://machinelearningmastery.com/how-to-perform-object-detection-with-yolov3-in-keras/)\n",
        "\n",
        "\n",
        "\n",
        "> [Github repo from experiencor](https://https://github.com/experiencor/keras-yolo3)\n",
        "\n",
        "Yolo v3 for Video practice was based on:\n",
        "> [Github repo from Ayoosh Kathuria  (Pytorch)](https://github.com/ayooshkathuria/pytorch-yolo-v3)\n",
        "\n",
        "\n",
        "All pictures taken from:\n",
        "\n",
        "1. [What‚Äôs new in YOLO v3? - Ayoosh Kathuria](https://towardsdatascience.com/yolo-v3-object-detection-53fb7d3bfe6b)\n",
        "\n",
        "\n",
        "2. [YOLO v3 theory explained - Analytics Vidhya](https://medium.com/analytics-vidhya/yolo-v3-theory-explained-33100f6d193)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CSR4S3x7_ze"
      },
      "source": [
        "# ...BUT how do I train a Yolo v3 on a custom dataset?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "1. Check [Ai Guy video Tutorial](https://www.youtube.com/watch?v=10joRJt39Ns) and try [his colab notebook](https://colab.research.google.com/drive/1Mh2HP_Mfxoao6qNFbhfV3u28tG8jAVGk)\n",
        "\n",
        "\n",
        "2.  Other good resource (but much less complete) is [Pysource video Tutorial](https://www.youtube.com/watch?v=_FNfRtXEbr4)\n",
        "\n",
        "\n",
        "Both are tutorials done using [the Darknet official (supported) repo](https://github.com/AlexeyAB/darknet) so are strongly recommended for practitioners\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}