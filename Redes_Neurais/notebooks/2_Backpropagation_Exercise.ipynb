{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZN4m7uHBMzE7"
      },
      "source": [
        "# Backpropagation\n",
        "Backpropagation (backprop, BP) is a widely used algorithm for training feedforward neural networks and is in the center of most of Deep Learning advances. During a neural network training, backpropagation computes the gradient of the loss function with respect to the weights of the network for a single input through the chain rule. The name backpropagation strictly refers only for computing the gradient, not how the gradient is used.\n",
        "\n",
        "Aiming at learning the underlaying concepts of Backpropagation, it is proposed an image classification exercise using the well-known [MNIST database](http://yann.lecun.com/exdb/mnist/) and an artificial neural network as classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ch-vo9Jg9wn9"
      },
      "source": [
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvUkN15-TcMg"
      },
      "source": [
        "#Loading the mnist datset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4N75Zki_2qK"
      },
      "source": [
        "(x_train,y_train), (x_test,y_test)= mnist.load_data()\n",
        "x_train = np.reshape(x_train, (-1, 784))\n",
        "x_test = np.reshape(x_test, (-1, 784))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5cK2UgUTRY3"
      },
      "source": [
        "#Dataset pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JR0amWgCAuBp"
      },
      "source": [
        "\n",
        "num_classes = 10\n",
        "# Dataset normalization\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train= x_train/255.\n",
        "x_test = x_test/255.\n",
        "# One-hot encoding\n",
        "y_train_h = np.zeros((np.size(y_train,0) , num_classes))\n",
        "y_test_h = np.zeros((np.size(y_test,0) , num_classes))\n",
        "for i in range(np.size(y_train , 0)):\n",
        "  y_train_h[i , y_train[i]] = 1\n",
        "\n",
        "for i in range(np.size(y_test , 0)):\n",
        "  y_test_h[i , y_test[i]] = 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCdIed6BS4Gw"
      },
      "source": [
        "#Parameters initialization functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQvcJbTyCgkw"
      },
      "source": [
        "\n",
        "def Weights_Init(size_1 , size_2):\n",
        "  W_init_max = 4 * np.sqrt(6. / (size_1 + size_2))\n",
        "  W = np.random.uniform(-W_init_max , W_init_max , (size_2 , size_1))\n",
        "  return W\n",
        "\n",
        "\n",
        "def Bias_Init(size):\n",
        "  b = np.zeros([size,1])\n",
        "  return b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2fmVpE0Scdz"
      },
      "source": [
        "#Network hyper-parameters configuration and trainable parameters initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QnNwNZJCJSg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1690f643-fadd-4ef5-c66d-ff1835773613"
      },
      "source": [
        "input_size = 784\n",
        "hidden_size = 256\n",
        "output_size = num_classes\n",
        "\n",
        "W1 = Weights_Init(input_size , hidden_size)\n",
        "W2 = Weights_Init(hidden_size, output_size)\n",
        "b1 = Bias_Init(hidden_size)\n",
        "b2 = Bias_Init(output_size)\n",
        "print(np.shape(W1))\n",
        "print(np.shape(b1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(256, 784)\n",
            "(256, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxfWjgEvRZfJ"
      },
      "source": [
        "#Forward propagation function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-Kkxk6jRcHp"
      },
      "source": [
        "\n",
        "def ForwardPropagation(X , W1, W2, b1, b2):\n",
        "  \n",
        "  X = np.transpose(X)\n",
        "  \n",
        "  a1 = np.concatenate((X , np.ones([1 , np.size(X , 1)])))\n",
        "  Wc1 = np.concatenate((W1 , b1),axis=1)\n",
        "  Wc2 = np.concatenate((W2 , b2),axis=1)\n",
        "  \n",
        "  a2 = 1/(1 + np.exp(np.matmul(-Wc1 , a1)))\n",
        "  a2 = np.concatenate((a2 , np.ones([1 , np.size(a2 , 1)])))\n",
        "  \n",
        "  a3 = 1/(1 + np.exp(np.matmul(-Wc2 , a2)))\n",
        "  return a3, a2, a1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ladrehEyReAN"
      },
      "source": [
        "# Loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h58SzAQdRZKe"
      },
      "source": [
        "def CrossEntropy(y , y_pred):  \n",
        "  y_pred = np.transpose(y_pred)\n",
        "  loss = np.mean(np.sum(-(y*np.log(y_pred) + (1 - y)*np.log(1 - y_pred)),1))\n",
        "  return loss\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oXm3TDxRh8e"
      },
      "source": [
        "#Accuracy score function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3xp6fVdsrjQ"
      },
      "source": [
        "def ComputeAccuracy(y_h , y_pred):\n",
        "  \n",
        "  y_pred = np.transpose(y_pred)\n",
        "  pred = np.argmax(y_pred,axis=1)\n",
        "  y = np.argmax(y_h, axis=1)\n",
        "  accuracy = accuracy_score(y , pred)\n",
        "  return accuracy\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uafjqn7URn1m"
      },
      "source": [
        "# BackPropagation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkuMEVxusvUI"
      },
      "source": [
        "def ComputeGrads(x_train_b, y_train_b, W1, b1, W2, b2):\n",
        "  \n",
        "  N = np.size(x_train_b, 0)\n",
        "  Wc1 = np.concatenate((W1 , b1),axis=1)\n",
        "  Wc2 = np.concatenate((W2 , b2),axis=1)\n",
        "  \n",
        "  y_pred, a2, a1 = ForwardPropagation(x_train_b, W1, W2, b1, b2)\n",
        "  delta3 = (y_pred-y_train_b.T)/N\n",
        "  delta2 = Wc2.T @ delta3 * (a2*(1-a2))\n",
        "  delta2 = delta2[:-1,:]\n",
        "\n",
        "  dWc2 = delta3 @ a2.T \n",
        "  dWc1 = delta2 @ a1.T\n",
        "  \n",
        "  return dWc1, dWc2\n",
        "  \n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sO7m4ALGRylq"
      },
      "source": [
        "#Training function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfbCTKtvWITK"
      },
      "source": [
        "num_epochs = 50\n",
        "batch_size = 100\n",
        "learning_rate = 0.001\n",
        "\n",
        "num_of_batchs_tr = np.size(x_train , 0)//batch_size\n",
        "num_of_batchs_vl = np.size(x_test  , 0)//batch_size\n",
        "for e in range(num_epochs):\n",
        "  x_train, y_train_h = shuffle(x_train, y_train_h, random_state = 0)\n",
        "  \n",
        "  #Computing the training error for each batch\n",
        "  loss_b = 0\n",
        "  acc_b = 0\n",
        "  for b in range(num_of_batchs_tr):\n",
        "    x_train_b = x_train[b*batch_size : (b + 1)*batch_size,:]\n",
        "    y_train_b = y_train_h[b*batch_size : (b + 1)*batch_size,:]\n",
        "    y_pred, _, _ = ForwardPropagation(x_train_b, W1, W2, b1, b2)\n",
        "    loss_b += CrossEntropy(y_train_b , y_pred)\n",
        "    acc_b += ComputeAccuracy(y_train_b , y_pred)\n",
        "  \n",
        "  loss_tr = loss_b/num_of_batchs_tr\n",
        "  acc_tr = acc_b/num_of_batchs_tr\n",
        "  \n",
        "  #Computing the validation error for each batch\n",
        "  loss_b = 0\n",
        "  acc_b = 0\n",
        "  for b in range(num_of_batchs_vl):\n",
        "    x_test_b = x_test[b*batch_size : (b + 1)*batch_size,:]\n",
        "    y_test_b = y_test_h[b*batch_size : (b + 1)*batch_size,:]\n",
        "    y_pred, _, _ = ForwardPropagation(x_test_b, W1, W2, b1, b2)\n",
        "    loss_b += CrossEntropy(y_test_b , y_pred)\n",
        "    acc_b += ComputeAccuracy(y_test_b , y_pred)\n",
        "    \n",
        "  loss_vl = loss_b/num_of_batchs_vl\n",
        "  acc_vl = acc_b/num_of_batchs_vl\n",
        "  print(\"Epoch: [%2d/%2d] Train loss: %.8f, Train accuracy: %.8f, Validation loss: %.8f, Validation accuracy: %.8f\" % (e+1, num_epochs, loss_tr, acc_tr*100, loss_vl, acc_vl*100))\n",
        "  #Performing backprop\n",
        "  for b in range(num_of_batchs_tr):\n",
        "    \n",
        "    x_train_b = x_train[b*batch_size : (b + 1)*batch_size, :]\n",
        "    y_train_b = y_train_h[b*batch_size : (b + 1)*batch_size, :]\n",
        "    dWc1, dWc2 = ComputeGrads(x_train_b, y_train_b, W1, b1, W2, b2)\n",
        "    #Update the parameters\n",
        "    W1 = W1 - learning_rate*dWc1[:,:-1]\n",
        "    W2 = W2 - learning_rate*dWc2[:,:-1]\n",
        "    b1 = b1 - learning_rate*np.reshape(dWc1[:,-1],(256,1))\n",
        "    b2 = b2 - learning_rate*np.reshape(dWc2[:,-1],(10,1))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}